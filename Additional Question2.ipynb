{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07301b08-43c5-4199-9f85-27dada445d00",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "A random forest is an ensemble of decision trees. In this exercise, write a python\r",
    "code that implements a random forest from scratch. Different trees are fitted\r",
    "with different subset of data (with all of the columns).\r",
    "1. Write a tree class that initializes a single decision tree.\r",
    "2. Write a random forest class that initializes n tree trees with different subsets\r",
    "of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e78a6-394f-490e-9666-c5b4793a107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=10, min_gain=0.01, max_leaf=100):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_gain = min_gain\n",
    "        self.max_leaf = max_leaf\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_single(x, self.tree) for x in X])\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        if depth >= self.max_depth or len(np.unique(y)) == 1:\n",
    "            return self._create_leaf(y)\n",
    "\n",
    "        best_split = self._find_best_split(X, y)\n",
    "        if best_split['gain'] < self.min_gain:\n",
    "            return self._create_leaf(y)\n",
    "\n",
    "        left_indices = X[:, best_split['feature']] <= best_split['threshold']\n",
    "        right_indices = X[:, best_split['feature']] > best_split['threshold']\n",
    "\n",
    "        left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {\n",
    "            'feature': best_split['feature'],\n",
    "            'threshold': best_split['threshold'],\n",
    "            'left': left_tree,\n",
    "            'right': right_tree\n",
    "        }\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_split = {'gain': -1}\n",
    "        n_samples, n_features = X.shape\n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(X[:, feature], y, threshold)\n",
    "                if gain > best_split['gain']:\n",
    "                    best_split = {\n",
    "                        'feature': feature,\n",
    "                        'threshold': threshold,\n",
    "                        'gain': gain\n",
    "                    }\n",
    "        return best_split\n",
    "\n",
    "    def _information_gain(self, feature_values, y, threshold):\n",
    "        parent_variance = self._variance(y)\n",
    "        left_indices = feature_values <= threshold\n",
    "        right_indices = feature_values > threshold\n",
    "        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        n_left = len(y[left_indices])\n",
    "        n_right = len(y[right_indices])\n",
    "\n",
    "        var_left = self._variance(y[left_indices])\n",
    "        var_right = self._variance(y[right_indices])\n",
    "\n",
    "        child_variance = (n_left / n) * var_left + (n_right / n) * var_right\n",
    "        ig = parent_variance - child_variance\n",
    "        return ig\n",
    "\n",
    "    def _variance(self, y):\n",
    "        return np.var(y)\n",
    "\n",
    "    def _create_leaf(self, y):\n",
    "        return np.mean(y)\n",
    "\n",
    "    def _predict_single(self, x, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "        feature = tree['feature']\n",
    "        threshold = tree['threshold']\n",
    "        if x[feature] <= threshold:\n",
    "            return self._predict_single(x, tree['left'])\n",
    "        else:\n",
    "            return self._predict_single(x, tree['right'])\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=10, min_gain=0.1, max_leaf=200):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_gain = min_gain\n",
    "        self.max_leaf = max_leaf\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for i in tqdm(range(self.n_trees), desc=\"Training Trees\"):\n",
    "            indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "            X_subset, y_subset = X[indices], y[indices]\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_gain=self.min_gain, max_leaf=self.max_leaf)\n",
    "            tree.fit(X_subset, y_subset)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in tqdm(self.trees, desc=\"Making Predictions\")])\n",
    "        return np.mean(tree_predictions, axis=0)\n",
    "\n",
    "\n",
    "def preprocess_data(df, continuous_features, n_bins=50):\n",
    "    kbd = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
    "    df[continuous_features] = kbd.fit_transform(df[continuous_features])\n",
    "    return df\n",
    "\n",
    "\n",
    "def grid_search_with_cv(X, y):\n",
    "\n",
    "    # first round of coarse grid search\n",
    "    param_grid = {\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'n_trees': [10, 30, 50]\n",
    "    }\n",
    "\n",
    "    # second round of coarse grid search\n",
    "    param_grid = {\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'n_trees': [40, 50, 60]\n",
    "    }\n",
    "\n",
    "    # third round of fine grid search\n",
    "    # impact of num_trees is not significant, use simple values so n_trees=10\n",
    "    param_grid = {\n",
    "        'max_depth': [8, 9, 10, 11],\n",
    "        'n_trees': [30]\n",
    "    }\n",
    "\n",
    "    # final model\n",
    "    param_grid = {\n",
    "        'max_depth': [10],\n",
    "        'n_trees': [30]\n",
    "    }\n",
    "\n",
    "    best_params = None\n",
    "    best_mse = float('inf')\n",
    "    results = []\n",
    "\n",
    "    kf = KFold(n_splits=5)\n",
    "\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for n_trees in param_grid['n_trees']:\n",
    "            model = RandomForest(n_trees=n_trees, max_depth=max_depth, min_gain=0.1, max_leaf=200)\n",
    "            mse_scores = []\n",
    "\n",
    "            for train_index, val_index in kf.split(X):\n",
    "                X_train, X_val = X[train_index], X[val_index]\n",
    "                y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_val)\n",
    "                mse_scores.append(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "            avg_mse = np.mean(mse_scores)\n",
    "            results.append((max_depth, n_trees, avg_mse))\n",
    "            print(f\"Params: max_depth={max_depth}, n_trees={n_trees} - MSE: {avg_mse}\")\n",
    "\n",
    "            if avg_mse < best_mse:\n",
    "                best_mse = avg_mse\n",
    "                best_params = (max_depth, n_trees)\n",
    "\n",
    "    print(f\"Best Params: max_depth={best_params[0]}, n_trees={best_params[1]} - MSE: {best_mse}\")\n",
    "    return best_params, results\n",
    "\n",
    "\n",
    "def visualize_grid_search(results):\n",
    "    max_depths = [r[0] for r in results]\n",
    "    n_trees = [r[1] for r in results]\n",
    "    mses = [r[2] for r in results]\n",
    "\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=max_depths, y=n_trees, z=mses,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=mses,\n",
    "            colorscale='Viridis',\n",
    "            colorbar=dict(title='MSE'),\n",
    "            opacity=0.8\n",
    "        )\n",
    "    )])\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='Max Depth',\n",
    "            yaxis_title='Number of Trees',\n",
    "            zaxis_title='MSE'\n",
    "        ),\n",
    "        title='Grid Search Results'\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the data\n",
    "    df = pd.read_csv('train_data2.csv', index_col=0)\n",
    "    continuous_features = ['macro_state_1', 'macro_state_2']\n",
    "    df = preprocess_data(df, continuous_features, n_bins=10)\n",
    "    X = df.drop(columns=['outcome']).values\n",
    "    y = df['outcome'].values\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    best_params, results = grid_search_with_cv(X, y)\n",
    "\n",
    "    # Visualize grid search results\n",
    "    visualize_grid_search(results)\n",
    "\n",
    "    # Train the model with the best parameters on the full dataset and save the model\n",
    "    model = RandomForest(n_trees=best_params[1], max_depth=best_params[0], min_gain=0.1, max_leaf=200)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Save the model to disk\n",
    "    with open('random_forest_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # Load the model from disk\n",
    "    with open('random_forest_model.pkl', 'rb') as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "    # Calculate the Mean Squared Error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error on Test Set: {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
